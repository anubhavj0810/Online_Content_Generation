{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbi:hide_in\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel, RBF\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "import numpy\n",
    "import nbinteract as nbi\n",
    "import seaborn as sns\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "train_data_source = '/Users/anubhavjain/Downloads/nlp_assignment_3/train.csv'\n",
    "test_data_source = '/Users/anubhavjain/Downloads/nlp_assignment_3/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_source, delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(test_data_source, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Rmjp_bmexlI0",
    "outputId": "0be4760a-b588-424e-b38c-5658229b2612"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta</th>\n",
       "      <th>uid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdilNisarButt</td>\n",
       "      <td>Hin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pakistan</td>\n",
       "      <td>Hin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ka</td>\n",
       "      <td>Hin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            meta  uid sentiment\n",
       "0           meta    3  negative\n",
       "1              @    O       NaN\n",
       "2  AdilNisarButt  Hin       NaN\n",
       "3       pakistan  Hin       NaN\n",
       "4             ka  Hin       NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "nglFKD6gPzuK",
    "outputId": "6b46e810-3103-4e30-b23e-57d36637f4f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta</th>\n",
       "      <th>uid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta</td>\n",
       "      <td>8</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT</td>\n",
       "      <td>Eng</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UAAPconfessions</td>\n",
       "      <td>Eng</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love</td>\n",
       "      <td>Eng</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              meta  uid sentiment\n",
       "0             meta    8   neutral\n",
       "1               RT  Eng       NaN\n",
       "2                @    O       NaN\n",
       "3  UAAPconfessions  Eng       NaN\n",
       "4             Love  Eng       NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Pm8gQh2LaF4",
    "outputId": "2f6ecd2e-b925-4cf7-e2ec-f5978a6dffd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ AdilNisarButt pakistan ka ghra tauq he Pakistan Israel ko tasleem nahein kerta Isko Palestine kehta he - OCCUPIED PALESTINE  negative\n"
     ]
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "string = \"\"\n",
    "a = train_df.loc[:,['meta','sentiment']]\n",
    "temp = \"\"\n",
    "train_arr = []\n",
    "train_arr_senti = []\n",
    "\n",
    "for i in range(a.shape[0]):\n",
    "  if(a.iloc[i,0]==\"meta\"):\n",
    "    #print(a.iloc[i,0])\n",
    "    train_arr.append(temp)\n",
    "    train_arr_senti.append(str(a.iloc[i,1]))\n",
    "    temp=\"\"\n",
    "  else:\n",
    "    temp+=str(a.iloc[i,0])+\" \"\n",
    "\n",
    "train_arr.append(temp)\n",
    "train_arr=train_arr[1:]\n",
    "print(train_arr[0],train_arr_senti[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w_7YZXfvPws6",
    "outputId": "f174fe2e-0b17-433f-8651-8231a70f8212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ UAAPconfessions Love looks good on Maddie !!! Ako lang ba yung sobrang masaya kasi may zolo sya ? Before with the past Z medyo lowkey s …  neutral\n"
     ]
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "string = \"\"\n",
    "a = test_df.loc[:,['meta','sentiment']]\n",
    "temp = \"\"\n",
    "test_arr = []\n",
    "test_arr_senti = []\n",
    "\n",
    "for i in range(a.shape[0]):\n",
    "  if(a.iloc[i,0]==\"meta\"):\n",
    "    #print(a.iloc[i,0])\n",
    "    test_arr.append(temp)\n",
    "    test_arr_senti.append(str(a.iloc[i,1]))\n",
    "    temp=\"\"\n",
    "  else:\n",
    "    temp+=str(a.iloc[i,0])+\" \"\n",
    "\n",
    "test_arr.append(temp)\n",
    "test_arr=test_arr[1:]\n",
    "print(test_arr[0],test_arr_senti[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens  337152\n",
      "No. of Types 65222\n",
      "TTR 0.19344983864844342\n"
     ]
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from nltk.corpus import wordnet\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "punc1 = string.punctuation\n",
    "punc = []\n",
    "for i in punc1:\n",
    "    punc.append(i)\n",
    "\n",
    "#print(punc, len(punc))\n",
    "#print(\"Default encoding is .. \",sys.getdefaultencoding())\n",
    "\n",
    "tknr = TweetTokenizer(strip_handles=True)\n",
    "sum = 0\n",
    "count_punc = 0\n",
    "arr = []\n",
    "\n",
    "heap_tok = []\n",
    "heap_voc = []\n",
    "c_tok = 0\n",
    "c_voc = 0\n",
    "s_voc = set()\n",
    "arr_tweet = []\n",
    "\n",
    "for i in train_arr:\n",
    "    a = tknr.tokenize(i)\n",
    "    b = []\n",
    "    arr1 = []\n",
    "    for j in a:\n",
    "        if(re.match(r\"\\.[\\s.]*\\.\",j)):\n",
    "            count_punc+=1\n",
    "        elif(j in punc):\n",
    "            count_punc+=1\n",
    "        elif(re.match(r\"https:\\/\\/\\S*|pic.twitter\\S*|http|Eng|Hin|positive|neutral|negative|RT|co\",j)):\n",
    "            count_punc+=1\n",
    "        else:\n",
    "            arr.append(j)\n",
    "            arr1.append(j)\n",
    "            b.append(j)\n",
    "            s_voc.add(j)\n",
    "    arr_tweet.append(\" \".join(arr1))\n",
    "    c_tok+=len(b)\n",
    "    c_voc= len(s_voc)\n",
    "    heap_tok.append(c_tok)\n",
    "    heap_voc.append(c_voc)\n",
    "    sum+=len(a)\n",
    "\n",
    "no_tokens = len(arr)\n",
    "print(\"No. of tokens \", no_tokens)\n",
    "\n",
    "type_arr = np.unique(arr)\n",
    "no_type = len(type_arr)\n",
    "\n",
    "print(\"No. of Types\", no_type)\n",
    "print(\"TTR\",no_type/no_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13524"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arr_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2500, description='num', max=5000), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "def word_map_generation(num):\n",
    "    all_words = ' '.join([text for text in arr_tweet[0:num]])\n",
    "    from wordcloud import WordCloud\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "word_map_generation = interact(word_map_generation, num=(0,5000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis is the automated process of analyzing text data and determining the sentiment of the opinion into positive, negative or neutral. Performing Sentiment Analysis on Twitter data can help companies obtain qualitative insights to understand how people are talking about their brand.\n",
    "\n",
    "With more than 321 million active users, sending a daily average of 500 million Tweets, Twitter has become one of the top social media platforms for news, information, and interaction with brands and influential figures around the world. Therefore, it is no surprise that companies consider this microblogging platform an essential channel for their marketing strategy and also, to provide customer service.\n",
    "\n",
    "Twitter allows businesses to reach a broad audience and connect with customers without intermediaries. On the downside, it can be harm a brand’s reputation if negative content about the brand suddenly goes viral – you might end up with an unexpected PR crisis on your hands. This is one of the reasons why social listening ― monitoring conversation and feedback in social media ― has become a crucial process in social media marketing.\n",
    "\n",
    "Monitoring Twitter allows companies to understand their audience, keep on top of what’s being said about their brand and their competitors, and discover new trends in the industry. However, when it comes to analyzing Twitter data, quantitative metrics like the number of mentions or retweets are not enough to get a full picture of a situation. What really counts is being able to grasp the nuance of those mentions. Are users talking positively or negatively about a product? And that’s exactly what sentiment analysis determines. It provides qualitative insights on what is being said about a topic or brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Sentiment Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis (a.k.a opinion mining) is the automated process of identifying and extracting the subjective information that underlies a text. This can be either an opinion, a judgment, or a feeling about a particular topic or subject. The most common type of sentiment analysis is called ‘polarity detection’ and consists in classifying a statement as ‘positive’, ‘negative’ or ‘neutral’.\n",
    "\n",
    "For example, let’s take this sentence: “I don’t find the app useful: it’s really slow and constantly crashing”. A sentiment analysis model would automatically tag this as Negative.\n",
    "A sub-field of Natural Language Processing (NLP), sentiment analysis has been getting a lot of attention in recent years due to its many exciting applications in a variety of fields, ranging from business to political studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is Twitter Sentiment Analysis important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly 80% of the world’s digital data is unstructured, and data obtained from social media sources is no exception to that. Since the information is not organized in any predefined way, it’s difficult to sort and analyze. Fortunately, thanks to the developments in Machine Learning and NLP, it is now possible to create models that learn from examples and can be used to process and organize text data.\n",
    "\n",
    "Twitter sentiment analysis systems allow you to sort large sets of tweets and detect the polarity of each statement automatically. And the best part, it’s fast and simple, saving teams valuable hours and allowing them to focus on tasks where they can make a bigger impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages of Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the main advantages of Twitter sentiment analysis:\n",
    "> **Scalability:** let’s say you need to analyze hundreds of tweets mentioning a brand. While you could do that manually, it would take hours and hours of manual processing and would end up being inconsistent and impossible to scale. By performing Twitter sentiment analysis you can automate this task and obtain cost-effective results in a very short time.\n",
    "\n",
    "> **Real-Time Analysis:** Twitter sentiment analysis is critical to notice sudden shifts in customer moods, detect if critics and complaints are increasing and take action before the problem escalates. You can be monitoring your brand in real-time and get valuable insights that allow you to make changes or improvements when needed.\n",
    "\n",
    "> **Consistent Criteria:** analyzing sentiment in a text is a subjective task. When done manually, the same tweet may be perceived differently by two members of the same team, and the results will probably be biased. By training a machine learning model to perform sentiment analysis on Twitter, you can set the parameters to analyze all your data and obtain more consistent and accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use Sentiment Analysis with Twitter Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify four main steps in this process:\n",
    "- **Data Preparing: Preprocessing and Cleaning Tweets**\n",
    "<br>\n",
    "Once you’ve captured the tweets you need for your sentiment analysis, it’s time to prepare the data. As we mentioned earlier, social media data is unstructured. That means it’s raw, noisy and needs to be cleaned before we can start working on our sentiment analysis model. This is an important step because the quality of the data will lead to more reliable results.\n",
    "Preprocessing a Twitter dataset involves a series of tasks like removing all types of irrelevant information like emojis, special characters, and extra blank spaces. It can also involve making format improvements, delete duplicate tweets, or tweets that are shorter than three characters.\n",
    "- **Creating a Twitter Sentiment Analysis Model**\n",
    "<br>\n",
    "Since the tweets dataset provided contains both Hindi and English mixed words, it was not possible to create embeddings at word level. So, the embeddings at character level were created. The total unique tokens were 900 after tokenization was done. Since the maximum number of characters of a tweet are 140, all tweets were post padded to make the maximum size of a tweet to length 150. The model has the following architecture which consists of Convolution1D, Maxpooling1D and Dense layers:\n",
    "- **Visualization of the results**\n",
    "<br>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
